{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, classification_report, confusion_matrix, f1_score, \\\n",
    "precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingRegressor, VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "# suppressing pesky warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from xgboost import XGBClassifier \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import catboost as cb\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# Visualisation\n",
    "#import geopandas as gpd\n",
    "#from geopandas import GeoDataFrame\n",
    "#from pyproj import CRS\n",
    "#from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will build and compare the following models, then choose one to use on the test set:\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- KNearest Neighbors\n",
    "- Catboost\n",
    "- XGBoost\n",
    "- Voting Classifier that incorporates above models\n",
    "\n",
    "The basic procedure for building each model is similar. First, we split the training data 80-20. Each model will test on the 20%, and the model that performs the best on that set will be used on the actual test set.\n",
    "\n",
    "For each model ,we create set of model parameters to iterate through and find the best set of parameters to fit the 80% training data using grid search, determined by best ROC_AUC values. We then save those models to a pickle file, using these models as inputs to the voting classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\", index_col = 0)\n",
    "\n",
    "X = train_data.iloc[:, 1:]\n",
    "y = train_data.iloc[:, 0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train), len(y_train), len(x_test), len(y_test), len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,25))\n",
    "cor = train_data.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomGrid():\n",
    "    \n",
    "    criterion = [\"gini\", \"entropy\"]\n",
    "    max_depth = range(1,5)\n",
    "    min_samples_split = range(2,10)\n",
    "    min_samples_leaf = range(1,10)\n",
    "    #min_samples_split = [2, 5, 10, 20]\n",
    "    #min_samples_leaf = [1, 20, 40, 100]\n",
    "    \n",
    "    random_grid = {\"criterion\": criterion ,\n",
    "             \"min_samples_split\": min_samples_split,\n",
    "              \"min_samples_leaf\": min_samples_leaf,\n",
    "              }\n",
    "        \n",
    "    #random_grid = {\"criterion\": criterion}\n",
    "    \n",
    "    return random_grid\n",
    "\n",
    "def runDecisionTree(random_grid, x_train, y_train):\n",
    "    \n",
    "    #rmse_score = make_scorer(rmse, greater_is_better = False)\n",
    "    \n",
    "    decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(decision_tree, random_grid, cv=10, scoring='roc_auc', n_jobs = -1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    final = grid_search.best_params_\n",
    "    print(final)\n",
    "    print(grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "param_set = createRandomGrid()\n",
    "best_model = runDecisionTree(param_set, x_train, y_train)\n",
    "best_model.fit(x_train,y_train)\n",
    "pickle.dump(best_model, open(\"DecisionTreeModel.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomGrid():\n",
    "    \n",
    "    n_neighbors = [2,5,10,15]\n",
    "    weights = ['uniform', 'distance']\n",
    "    metric = ['euclidean', 'manhattan']\n",
    "    \n",
    "    random_grid = {'n_neighbors': n_neighbors, \n",
    "                   'weights': weights,\n",
    "                  'metric': metric}\n",
    "\n",
    "    \n",
    "    \n",
    "    return random_grid\n",
    "\n",
    "def runKNeighbors(random_grid, x_train, y_train):\n",
    "    \n",
    "#     rmse_score = make_scorer(rmse, greater_is_better = False)\n",
    "    \n",
    "    K_neigh = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(K_neigh, random_grid, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    final = grid_search.best_params_\n",
    "    print(final)\n",
    "    print(grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "param_set = createRandomGrid()\n",
    "best_model = runKNeighbors(param_set, x_train, y_train)\n",
    "best_model.fit(x_train,y_train)\n",
    "pickle.dump(best_model, open(\"KNeighbors.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomGrid():\n",
    "    n_estimators = [10 , 50, 100, 200, 400, 500]\n",
    "    max_features = [6,8,10,12, 14 ]\n",
    "    max_depth = [2] \n",
    "    min_samples_leaf = [1,5,10,25,50]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'bootstrap': bootstrap,\n",
    "                    'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_leaf': min_samples_leaf\n",
    "                                    }\n",
    "    \n",
    "    return random_grid\n",
    "\n",
    "def runRandomForest(random_grid, x_train, y_train):\n",
    "    \n",
    "    forest = RandomForestClassifier(random_state = 42)\n",
    "    grid_search = GridSearchCV(forest, random_grid, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    final = grid_search.best_params_\n",
    "    print(final)\n",
    "    print(grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "param_set = createRandomGrid()\n",
    "best_model = runRandomForest(param_set, x_train, y_train)\n",
    "best_model.fit(x_train,y_train)\n",
    "pickle.dump(best_model, open(\"RandomForestModel.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc = RandomForestClassifier()\n",
    "# rfc.fit(x_train, y_train)\n",
    "# y_pred_prob = rfc.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.round(np.linspace(0.05, 0.95, 19), 3):\n",
    "#     print('#########################')\n",
    "#     print('Threshold =', i)\n",
    "#     yp = np.where(y_pred_prob >= i, 1, 0)\n",
    "#     print(classification_report(y_test, yp), confusion_matrix(y_test, yp), roc_auc_score(y_test, yp))\n",
    "#     print('#########################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomGrid():\n",
    "        \n",
    "        random_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "    \n",
    "        return random_grid\n",
    "\n",
    "def runXGB(random_grid, x_train, y_train):\n",
    "    \n",
    "    xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    grid_search = GridSearchCV(xgb, random_grid, cv=5, scoring='roc_auc', n_jobs = 4, verbose=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    final = grid_search.best_params_\n",
    "    print(final)\n",
    "    print(grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "param_set = createRandomGrid()\n",
    "best_model = runXGB(param_set, x_train, y_train)\n",
    "best_model.fit(x_train,y_train)\n",
    "pickle.dump(best_model, open(\"XGBModel.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomGrid():\n",
    "        \n",
    "        random_grid = {\n",
    "        'learning_rate': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'depth': [1,2,3,4,5,6,7],\n",
    "        'l2_leaf_reg': [1,2,3,4,5,6,7,8,9,10]\n",
    "        }\n",
    "\n",
    "    \n",
    "        return random_grid\n",
    "\n",
    "def runCat(random_grid, x_train, y_train):\n",
    "    \n",
    "    cat = cb.CatBoostClassifier(silent=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    grid_search = GridSearchCV(cat, random_grid, cv=5, scoring='roc_auc', n_jobs = 4, verbose=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    final = grid_search.best_params_\n",
    "    print(final)\n",
    "    print(grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "param_set = createRandomGrid()\n",
    "best_model = runCat(param_set, x_train, y_train)\n",
    "best_model.fit(x_train,y_train)\n",
    "pickle.dump(best_model, open(\"CatModel.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_rf_model = pickle.load(open(\"RandomForestModel.pkl\", 'rb'))\n",
    "loaded_kn_model = pickle.load(open(\"KNeighbors.pkl\", 'rb'))\n",
    "loaded_dt_model = pickle.load(open(\"DecisionTreeModel.pkl\", 'rb'))\n",
    "loaded_xg_model = pickle.load(open(\"XGBModel.pkl\", 'rb'))\n",
    "loaded_cb_model = pickle.load(open('CatModel.pkl', 'rb'))\n",
    "\n",
    "\n",
    "vtr = VotingClassifier([('rf', loaded_rf_model), ('kn', loaded_kn_model), ('dt', loaded_dt_model), ('xg', loaded_xg_model),\n",
    "                        ('cb', loaded_cb_model)])\n",
    "vtr.fit(x_train,y_train)\n",
    "pickle.dump(vtr, open(\"VotingClassifierModel.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Here we will load the optimal models we created and run them on our test set. For each model we calculate the following metrics:\n",
    "\n",
    "- ROC_AUC Score\n",
    "- Brier Score\n",
    "- Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Models\n",
    "\n",
    "rf_model = pickle.load(open(\"RandomForestModel.pkl\", 'rb'))\n",
    "kn_model = pickle.load(open(\"KNeighbors.pkl\", 'rb'))\n",
    "dt_model = pickle.load(open(\"DecisionTreeModel.pkl\", 'rb'))\n",
    "vtr_model = pickle.load(open(\"VotingClassifierModel.pkl\", 'rb'))\n",
    "xg_model = pickle.load(open(\"XGBModel.pkl\", 'rb'))\n",
    "cb_model = pickle.load(open('CatModel.pkl', 'rb'))\n",
    "\n",
    "#x_train = pickle.load(open(\"SplitTrainX_V2.pkl\", 'rb'))\n",
    "#x_test = pickle.load(open(\"SplitTestX_V2.pkl\", 'rb'))\n",
    "#y_train = pickle.load(open(\"SplitTrainY_V2.pkl\", 'rb'))\n",
    "#y_test = pickle.load(open(\"SplitTestY_V2.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(y_test, y_pred):\n",
    "    return (1 / len(y_pred)) * sum((y_pred - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through models and running them on test set\n",
    "\n",
    "model_list = [rf_model, kn_model, dt_model, vtr_model,xg_model, cb_model]\n",
    "model_names = [\"RandomForest\", \"KNearestNeighbors\", \"DecisionTree\", \"VotingClassifier\", \"XGBoost\", 'CatBoost']\n",
    "roc_auc_list = []\n",
    "results_df_list = []\n",
    "for i in range(6):\n",
    "    model = model_list[i]\n",
    "    model_name = model_names[i]\n",
    "    y_test_predictions =  model.predict(x_test)\n",
    "    y_train_predictions = model.predict(x_train)\n",
    "\n",
    "#     rmse_test = math.sqrt(mean_squared_error(y_test, y_test_predictions))\n",
    "#     rmse_train = math.sqrt(mean_squared_error(y_train, y_train_predictions))\n",
    "    roc_auc_test = roc_auc_score(y_test, y_test_predictions)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_train_predictions)\n",
    "    cv_scores = cross_val_score(model, x_train, y_train, cv=10, scoring='roc_auc', n_jobs=-1)\n",
    "    roc_auc_cv = cv_scores.mean()\n",
    "    f1 = f1_score(y_test, y_test_predictions)\n",
    "    if(model_name == 'VotingClassifier'):\n",
    "        brier = np.nan\n",
    "    else:\n",
    "        y_test_pred_prob = model.predict_proba(x_test)[:, 1]\n",
    "        brier = brier_score(y_test, y_test_pred_prob)\n",
    "    precision_test = precision_score(y_test, y_test_predictions)\n",
    "    recall_test = recall_score(y_test, y_test_predictions)\n",
    "    cf = confusion_matrix(y_test, y_test_predictions)\n",
    "    cf = cf.flatten()\n",
    "\n",
    "    roc_auc_list.append([model_name,roc_auc_train,roc_auc_cv, roc_auc_test, f1, brier, precision_test, recall_test, \n",
    "                         cf[0], cf[1], cf[2], cf[3]])\n",
    "    \n",
    "    y_test = np.array(list(y_test))\n",
    "    y_test_predictions = np.array(y_test_predictions)\n",
    "    dfi = pd.DataFrame({'Actual IR': y_test, 'Predicted IR': y_test_predictions})\n",
    "    results_df_list.append(dfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_df = pd.DataFrame(roc_auc_list, columns = [\"Model Name\", \"Train ROC AUC\", \"CV ROC AUC\", \"Test ROC AUC\", \n",
    "                                                   'Test F1 Score', 'Brier Score', 'Test Precision', 'Test Recall', \n",
    "                                                   'Test TN', 'Test FN', 'Test FP', 'Test TP'])\n",
    "roc_auc_df.sort_values(by=['Test ROC AUC'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = xg_model.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.round(np.linspace(0.05, 0.95, 19), 3):\n",
    "    print('#########################')\n",
    "    print('Threshold =', i)\n",
    "    yp = np.where(y_pred_prob >= i, 1, 0)\n",
    "    print(classification_report(y_test, yp), confusion_matrix(y_test, yp), roc_auc_score(y_test, yp), recall_score(y_test, yp))\n",
    "    print('#########################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
